OpenAI är ett amerikanskt företag som grundades med målet att främja och utveckla artificiell intelligens (AI) på ett sätt som gynnar mänskligheten. Företaget arbetar med forskning och utveckling inom områden som maskininlärning, neuronnät och AI-etik för att möjliggöra innovativ och ansvarsfull användning av AI-teknik. OpenAI har skapat chatboten ChatGPT som bygger på GPT-3, vilket är en språkmodell som används för att generera texter med naturligt språk.
Sam Altman var ordförande i OpenAI mellan åren 2015-2023. Ilya Sutskever, tidigare forskare vid Google, är forskningschef. Greg Brockman, Tidigare CTO för Stripe, är CTO. Företaget stöds av 1 miljard amerikanska dollar, bland annat genom Reid Hoffman, Jessica Livingston, Peter Thiel, Amazon Web Services, Infosys och YC Research.
Den 17 november 2023 avsattes Sam Altman som CEO av OpenAIs styrelse då de saknade förtroende för honom. Kort därefter valde Greg Brockman att sluta. Som tillfällig CEO installerades Mira Murati som tidigare arbetade som CTO. Den 22 november 2023 återvände Altman till posten som företagets VD. 


== Produkter och applikationer ==
OpenAI:s forskning fokuserar på förstärkningsinlärning. OpenAI ses som en konkurrent till Deepmind.


=== ChatGPT ===

ChatGPT är en avancerad AI-språkmodell som bygger på GPT-3 och har förmågan att generera naturlig och sammanhängande text baserat på inmatad text. Användare kan ställa en fråga och få svar inom några sekunder. ChatGTP lanserades i november 2022 och hade 1 miljon användare efter fem dagar. 
Modellen är tränad genom övervakad inlärning (supervised learning) med stora mängder data från internet. Detta ger ChatGPT en bred kunskapsbas och förmåga att förstå och svara på olika frågor, ge förslag och utföra olika språkbaserade uppgifter. ChatGPT har användningar inom bland annat kundsupport, programmering och undervisning.


=== Gym ===
Gym syftar till att tillhandahålla benchmarks för artificiell generell intelligens (AGI) som är lätta att installera i en mängd olika miljöer – något liknande men bredare än ImageNet, som används i forskning inom övervakad inlärning. Tanken är att kunna standardisera hur miljöer definieras i AI-forskningspublikationer, så att publicerad forskning blir enklare att reproducera. Projektet gör anspråk på att ge användaren ett enkelt gränssnitt. Från och med juni 2017 kan Gym endast användas med Python. Från och med september 2017 uppdateras inte dokumentationssidan för Gym, utan aktuell information finns istället på deras Github-sida.


=== Robosumo ===
Robosumo är en virtuell värld där agenter i form av virtuella humanoida robotar (agenter) initialt saknar kunskap om hur man ens går, men agenterna får sen målet att lära sig att röra på sig och knuffa ut en motståndagent ur ringen, som i sumobrottning. Genom denna "fientliga" inlärningsprocess lär sig agenterna hur man anpassar sig till förändrade förhållanden; när en agent sedan avlägsnas från denna virtuella miljö och placeras i en ny virtuell miljö med t.ex. kraftiga vindar, ställer sig agenten upp för att förbli upprätt, vilket tyder på att den hade lärt sig att balansera på ett generaliserat sätt. OpenAI:s Igor Mordatch hävdar att konkurrens mellan agenter kan skapa en intelligens "kapprustning" som kan öka en agents förmåga att fungera, även utanför tävlingens sammanhang.


=== Debattspel ===
2018 lanserade OpenAI Debate Game, som lär maskiner att diskutera låtsasproblem inför en mänsklig domare. Syftet är att undersöka om ett sådant tillvägagångssätt kan hjälpa vid granskning av AI-beslut och att utveckla förklarlig AI.


=== Dactyl ===
Dactyl använder maskininlärning för att träna en robothand, Shadow Hand, från grunden, med samma kod för förstärkningsinlärning som OpenAI Five använder. Robothanden tränas helt i fysiskt inexakt simulering.2019 visade OpenAI att Dactyl kunde lösa Rubiks kub 60% av gångerna den försökte.


=== Generativa modeller ===


=== GPT ===
Den ursprungliga artikeln om generativ förträning (GPT) av en språkmodell skrevs av Alec Radford och kollegor och publicerades i preprint på OpenAI:s webbplats den 11 juni 2018. Den visade hur en generativ språkmodell kan förvärva världskunskap och bearbeta långväga beroenden genom förträning på en mångsidig korpus med stora mängder av lång sammanhängande text.


=== GPT-2 ===
Generative Pre-trained Transformer 2, allmänt känd genom sin förkortade form GPT-2, är en oövervakad transformatorspråkmodell och efterföljaren till GPT. GPT-2 tillkännagavs först i februari 2019, med endast begränsade demonstrationsversioner som initialt släpptes för allmänheten. Den fullständiga versionen av GPT-2 släpptes inte omedelbart på grund av oro för potentiellt missbruk, t.ex. att den skulle tillämpas för att skriva falska nyheter. Vissa experter uttryckte skepsis mot att GPT-2 utgjorde ett betydande hot. Allen Institute for Artificial Intelligence gjorde motdraget att bygga ett verktyg som kan användas för att upptäcka "neurala falska nyheter". Andra forskare, som Jeremy Howard, varnade för "tekniken för att helt och hållet fylla Twitter, e-post och webben med rimligt klingande, kontextanpassad prosa, som skulle dränka allt annat tal och vara omöjligt att filtrera". I november 2019 släppte OpenAI den fullständiga versionen av språkmodellen GPT-2. Flera webbplatser är värdar för interaktiva demonstrationer av olika instanser av GPT-2 och andra transformatormodeller.Radfort et al. hävdar att oövervakade språkmodeller är "general-purpose learners", vilket illustreras av att GPT-2 uppnår hög precision och perplexitet på 7 av 8 zero-shot uppgifter (dvs modellen tränades inte vidare på någon uppgiftsspecifik in-utdata). Korpusen den utbildades på, kallad Webtext, innehåller drygt 8 miljoner dokument för totalt 40 GB text från webbadresser som delas i Reddit-trådar med minst 3 gillamarkeringar. Det gör att man undviker problem med att koda vokabulär med ordtokens genom att använda byteparkodning. Detta gör det möjligt att representera vilken sträng av tecken som helst, genom att koda både individuella tecken och tokens med flera tecken.


=== GPT-3 ===

Generative Pre-trained Transformer 3, allmänt känd genom sin förkortade form GPT-3, är en oövervakad Transformer-språkmodell och efterföljaren till GPT-2. Den omtalades första gången i maj 2020. OpenAI uppgav att den fullständiga versionen av GPT-3 innehåller 175 miljarder parametrar, två storleksordningar större än de 1,5 miljarder parametrar i den fullständiga versionen av GPT-2 (även om GPT-3-modeller med så få som 125 miljoner parametrar tränades också).
OpenAI uppgav att GPT-3 lyckats med vissa "meta-lärande"-uppgifter. Det kan generalisera syftet med ett enda in-out-par. Brown et al. ger ett exempel på översättning och tvärspråklig överföring av lärande mellan engelska och svenska, och mellan engelska och tyska..GPT-3 förbättrade avsevärt benchmarkresultat jämfört med GPT-2. OpenAI varnade för att en sådan uppskalning av språkmodeller kan närma sig eller stöta på de grundläggande kapacitetsbegränsningarna hos prediktiva språkmodeller. Förträning av GPT-3 krävde flera tusen petaflop/s-dagar beräkning, jämfört med tiotals petaflop/s-dagar för hela GPT-2-modellen. Liksom sin föregångare släpptes GPT-3:s fullt utbildade modell inte omedelbart till allmänheten på grund av eventuellt missbruk, även om OpenAI planerade att tillåta åtkomst via ett betald moln-API efter en två månader lång gratis privat beta som började i juni 2020.Den 23 september 2020 licensierades GPT-3 exklusivt till Microsoft.


=== Musik ===
OpenAIs Musenet (2019) är ett djupt neuralt nät som tränats för att förutsäga efterföljande musikaliska toner i MIDI-musikfiler. Den kan generera låtar med tio olika instrument i femton olika stilar. Enligt The Verge tenderar en låt som genereras av Musenet att börja rimligt men sedan hamna i kaos ju längre den spelas.OpenAIs Jukebox (2020) är en öppen källkodsalgoritm för att generera musik med sång. Efter att ha tränat på 1,2 miljoner samplingar, accepterar systemet en genre, artist och ett stycke text och matar ut låtprover. OpenAI konstaterade att låtarna "visar lokal musikalisk koherens, följer traditionella ackordmönster" men erkände att låtarna saknar "bekanta större musikaliska strukturer som refränger som upprepas" och att "det finns en betydande klyfta" mellan Jukebox och mänskligt genererad musik. The Verge sa "Det är tekniskt imponerande, även om resultaten låter som sörjiga versioner av låtar som kan kännas bekanta", medan Business Insider sa "överraskande nog är några av de resulterande låtarna catchy och låter legitima".


=== API ===
I juni 2020 tillkännagav OpenAI ett multifunktions-API som det sa var "för att komma åt nya AI-modeller utvecklade av OpenAI" för att låta utvecklare kalla på det för "alla engelska AI-uppgifter".


=== DALL-E och CLIP ===
DALL-E är en transformatormodell som skapar bilder från textbeskrivningar, avslöjade av OpenAI i januari 2021.CLIP gör tvärtom: det skapar en beskrivning för en given bild. DALL-E använder en version med 12 miljarder parametrar av GPT-3 för att tolka naturliga språkinmatningar (som "en grön läderväska formad som en femhörning" eller "en isometrisk vy av en sorglig kapybara") och generera motsvarande bilder. Den kan skapa bilder av realistiska föremål ("ett målat glasfönster med en bild av en blå jordgubbe") såväl som föremål som inte finns i verkligheten ("en kub med strukturen av ett piggsvin"). Från och med mars 2021 finns inget API eller kod tillgänglig.


=== Microscope ===
OpenAI Microscope är en samling visualiseringar av varje betydande lager och neuron av åtta olika neurala nätverksmodeller som ofta studeras i tolkningsbarhet. Microscope skapades för enkel analys av funktionerna som bildas i dessa neurala nätverk. Modellerna som ingår är Alexnet, VGG 19, olika versioner av Inception och olika versioner av CLIP Resnet.


=== Codex ===
OpenAI Codex är en ättling till GPT-3 som dessutom har tränats på kod från 54 miljoner Github-repositories. Det tillkännagavs i mitten av 2021 som AI som driver kodautokompletteringsverktyget Github Copilot. I augusti 2021 släpptes ett API i privat beta. Enligt OpenAI kan modellen skapa arbetskod i över ett dussin programmeringsspråk, mest effektivt i Python.Flera problem med fel, designbrister och säkerhetsbrister har tagits upp.


== Debaclet november 2023 ==
Enligt Max Tegmark, i Svt:s Aktuellt den 23 nov 2023, handlade den då aktuella Open AI-debatten om: Ska företagens girighet få styra AI, eller måste inte AI:s säkerhet nu regleras mer (exempelvis som avseende läkemedel i Sverige). Riskerna kan vara att terrorister utnyttjar nästa generations AI exempelvis för att framställa biologiska vapen, att människors arbeten görs bättre och billigare av AI, samt att man låter bygga maskiner som är bättre än "oss", istället för att "vi" ser till att AI hjälper människorna.


== Referenser ==
Den här artikeln är helt eller delvis baserad på material från engelskspråkiga Wikipedia, OpenAI, 23 juni 2016.


=== Noter ===


== Externa länkar ==
Open AI:s webbplats