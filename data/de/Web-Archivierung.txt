"web-archivierung bezeichnet das sammeln und dauerhafte ablegen von netzpublikationen mit dem zweck, in der zukunft \u00f6ffentlichkeit und wissenschaft einen blick in die vergangenheit bieten zu k\u00f6nnen. ergebnis des vorgangs ist ein web-archiv.\ndie gr\u00f6\u00dfte internationale einrichtung zur web-archivierung ist das internet archive in san francisco (usa), das sich als archiv des gesamten world wide web versteht. staatliche archive und bibliotheken in vielen l\u00e4ndern unternehmen anstrengungen zur sicherung der netz\u00fcberlieferung in ihrem bereich.\ndie deutschen archivgesetze definierten ab 1987 die archivierung digitaler unterlagen als pflichtaufgabe der staatlichen archive, die umsetzung dieses auftrags l\u00e4uft aber erst an. im jahr 2006 wurde das dnbg (gesetz zur deutschen nationalbibliothek) verabschiedet, das den auftrag der deutschen nationalbibliothek auf die archivierung von websites ausdehnt. auch die bundesl\u00e4nder planen, ihre pflichtexemplar-gesetze in diesem sinne zu \u00e4ndern, oder haben die \u00e4nderung bereits vollzogen.\n\n\n== archivierungsziele ==\nweb-archivierung verfolgt das ziel, einen definierten ausschnitt der im internet vorhandenen web-pr\u00e4senzen in systematischer form abzubilden. hierf\u00fcr sind eine \u00fcbergreifende sammlungspolitik, ein auswahlverfahren und die h\u00e4ufigkeit der archivierung vorab zu kl\u00e4ren.\neine archivierte website sollte mit allen multimedialen funktionen (html-code, stylesheets, javascript, bilder und video) auf dauer erhalten werden. der sp\u00e4teren beschreibung, nutzung und erhaltung dienen metadaten wie provenienz, \u00fcbernahmezeitpunkt, mime-type und umfang der daten. die metadaten sichern authentizit\u00e4t und integrit\u00e4t der digitalen archivalien.\nnach der \u00fcbernahme sind technische und juristische vorkehrungen zu treffen, um eine st\u00e4ndige \u00f6ffentliche zug\u00e4nglichkeit zu garantieren und eine nachtr\u00e4gliche ver\u00e4nderung der archivalien zu verhindern.\n\n\n== begrifflichkeiten ==\noriginal resource\neine originale quelle, die aktuell im internet vorhanden ist oder vorhanden sein sollte und f\u00fcr die ein zugriff auf einen fr\u00fcheren zustand ben\u00f6tigt wird.\nmemento\nein memento einer originalen quelle ist eine ressource, die den originalen zustand einer quelle zu einem definierten zeitpunkt kapselt.\ntimegate\nein timegate ist eine ressource, die auf basis eines vorgegebenen datums und einer zeitangabe jenes memento findet, welches dieser zeitlichen vorgabe am besten entspricht.\ntimemap\neine timemap ist eine ressource, welche eine liste aller mementos ausgibt, die f\u00fcr die originale quelle je angelegt wurden.\n\n\n== auswahlverfahren ==\nunspezifisch\nbei diesem auswahlverfahren wird eine ganze domain nach und nach in ein archiv geschrieben. das verfahren funktioniert wegen des gro\u00dfen speicherbedarfs nur bei kleineren domains (netarkivet.dk).\nauswahlliste\neine liste von institutionen wird vorab festgelegt. die stabilit\u00e4t der mit den institutionen verbundenen urls ist regelm\u00e4\u00dfig zu pr\u00fcfen.\nnutzung von zugriffsstatistiken\nin zukunft ist ein \u201eintelligentes\u201c harvesting (deutsch \u201eernten\u201c) denkbar, das aufgrund von zugriffsz\u00e4hlungen diejenigen teile des web (oder einer auswahl) archiviert, die besonders hohe zugriffsraten aufweisen.\n\n\n== \u00fcbernahmemethoden ==\n\n\n=== remote harvesting ===\ndie \u00fcblichste archivierungsmethode ist die nutzung eines webcrawlers. ein web-crawler ruft die inhalte einer website wie ein menschlicher nutzer ab und schreibt die ergebnisse in ein archivobjekt. genauer betrachtet bedeutet das ein rekursives durchsuchen von webseiten anhand der darauf gefundenen links, ausgehend von einem gewissen startbereich, der entweder eine webseite oder auch eine liste an webseiten, die durchsucht werden sollen, sein kann. aufgrund mengenm\u00e4\u00dfiger limitationen, etwa wegen dauer oder speicherplatz, sind diverse einschr\u00e4nkungen (abbruchbedingungen) hinsichtlich tiefe, domain und der zu archivierenden dateiarten m\u00f6glich.\nbei gr\u00f6\u00dferen projekten kommt hierbei der bewertung von webseiten zur url-reihung eine besondere bedeutung zu. im verlauf eines crawl-vorganges k\u00f6nnen sich unter umst\u00e4nden sehr viele webadressen ansammeln, die dann entweder in einer liste nach dem fifo-verfahren oder als priorit\u00e4tswarteschlange abgearbeitet werden. f\u00fcr letzteren fall kann man sich die webseiten dabei in einer heap-struktur vorstellen. jede webseite an sich bildet einen eigenen heap und jeder darin gefundene link zu einer weiteren webseite bildet wiederum einen unterheap, der ein element im heap der vorhergehenden webseite darstellt. das hat auch den vorteil, dass im fall einer \u00fcberlaufenden url-liste zuerst diejenigen mit der niedrigsten priorit\u00e4t durch neue eintr\u00e4ge ersetzt werden.\ndie ausgangsstruktur auf dem server l\u00e4sst sich allerdings im archiv nur selten exakt nachbauen. um bereits im vorfeld einer spiegelung eventuell auftretende technische probleme ausschlie\u00dfen zu k\u00f6nnen, bietet es sich an, vorab eine analyse der webseite durchzuf\u00fchren. dies verdoppelt zwar in den meisten f\u00e4llen den datenverkehr, verk\u00fcrzt aber die aufzuwendende arbeitszeit im fehlerfall erheblich.beispiele f\u00fcr webcrawler sind:\n\nheritrix\nhttrack\noffline explorer\n\n\n=== archivierung des hidden web ===\ndas hidden web oder deep web bezieht sich auf datenbanken, die oft die eigentlichen inhalte einer website repr\u00e4sentieren und nur auf anfrage eines nutzers ausgegeben werden. auch dadurch \u00e4ndert sich das web st\u00e4ndig und es erscheint, als w\u00fcrde dieses eine unendliche gr\u00f6\u00dfe besitzen. zur \u00fcbernahme dieser datenbanken ist eine schnittstelle erforderlich, die meist auf xml beruht. f\u00fcr einen solchen zugang sind die tools deeparc (biblioth\u00e8que nationale de france) und xinq (national library of australia) entwickelt worden.\n\n\n=== transactional archiving ===\ndieses verfahren dient der archivierung der ergebnisse eines nutzungsprozesses von websites. es ist f\u00fcr einrichtungen von bedeutung, die aus rechtlichen gr\u00fcnden einen nachweis \u00fcber ihre nutzung zu f\u00fchren haben. voraussetzung ist die installation eines zusatzprogramms auf dem webserver.\n\n\n== webarchivierung in deutschland ==\nauf bundesebene hat die deutsche nationalbibliothek (dnb) seit 2006 den gesetzlichen auftrag zur webarchivierung. seit 2012 werden webseiten thematisch und bei bestimmten ereignissen archiviert, also selektiv und nicht vollumf\u00e4nglich. die dnb arbeitet dabei mit einem externen dienstleister zusammen. au\u00dferdem wurden 2014 bisher einmalig alle de-domains gecrawlt. der zugriff auf das webarchiv erfolgt haupts\u00e4chlich in den leses\u00e4len.neben der webarchivierung der dnb gibt es in verschiedenen bundesl\u00e4ndern initiativen:\n\nbaden-w\u00fcrttemberg: das baden-w\u00fcrttembergische online-archiv (boa) sammelte bis 2019 digitale publikationen und ausgew\u00e4hlte webseiten. seit 2020 benutzt das landesarchiv baden-w\u00fcrttemberg das dimag-modul diwi, f\u00fcr die bibliothekarischen aufgaben hat das bibliotheksservice-zentrum den dienst archive-it des internet archive beauftragt.\nbayern: die bayerische staatsbibliothek sammelt seit 2010 ausgew\u00e4hlte webseiten.\nrheinland-pfalz: die rheinische landesbibliothek sammelt seit 2003 im projekt edoweb ausgew\u00e4hlte webseiten.au\u00dferdem gibt es in deutschland weitere webarchivierungsinitiativen beispielsweise von parteinahen stiftungen, vom swr, von der deutschen post oder vom biotechnologie-/pharmaunternehmen abbvie.\n\n\n== siehe auch ==\ncompliant transaction recording\ncontent-lifecycle\nelektronische archivierung\nlangzeitarchivierung\nopen archives initiative\nweb archive\n\n\n== umsetzungen ==\ndie \u201ewayback machine\u201c des internet archives\narchive.is\ndimag\nwebcite\ngoogle-cache (die letzte version einer web-adresse in der f\u00fcr google-server optimierten variante wird f\u00fcr einige wochen bereit gestellt)\n\n\n== weblinks ==\ntime travel (metasuche in ca. 25 internet-archiven)\ngyo/megalodon (japanische metasuche in internet-archiven)\ninternational internet preservation consortium (iipc) \u2013 internationales konsortium mit der aufgabe, informationen und wissen aus dem internet f\u00fcr k\u00fcnftige generationen zu \u00fcbernehmen, zu erhalten und zug\u00e4nglich zu machen\ninternational web archiving workshop (iwaw) \u2013 j\u00e4hrlich stattfindender workshop zur web-archivierung\ndigital collections and programs. library of congress.\nweb archiving. library of congress.\nweb archiving bibliography. tuwien.ac.at; literaturliste zur web-archivierung.\nweb archiving discussion list. cru.fr; diskutiert technische, organisatorische und rechtliche fragen der web-archivierung.\nliteratur von michael l. nelson \u2013 wissenschaftliche artikel auf dblp.uni-trier.de\n\n\n== einzelnachweise =="